# N_gram_
В данном задании я реализовал n_gram_model путем сокращения длины контекста.

## process_text.py:
Обрабатывает суммарный текст трех книг и сохраняет данные для модели в файле text.
## Train.py:
Считает статистику вероятности встречи слова в составе n-grama, в модели используются n_gramы начиная от размера 5 до 2.
Вся статистика сохраняется в output.dill.
## generate.py:
Генерирует текст заданной длины. По умолчанию обращается к вероятностям встречи слов с самым длинным контекстом. 
Если не находит, то к - меньшей длине контекста.
Результат сохранается в файле generated_text.


